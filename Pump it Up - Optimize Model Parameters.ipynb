{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting faulty water pumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Model Paramaters¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "Import libraries necessary to get the models trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "from IPython.display import display # use of display() for DataFrames\n",
    "\n",
    "# Import Preprocessing and ML libraries\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, accuracy_score, f1_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Water Pump Data Files, Create Data Frames and Clean The Data\n",
    "\n",
    "#### Read in the data pump files stored in the directory raw-data into Pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive E is DATA\n",
      " Volume Serial Number is 6E98-6AE7\n",
      "\n",
      " Directory of E:\\GitHub\\Udacity\\machine-learning\\projects\\capstone-project\\raw-data\n",
      "\n",
      "05/31/2018  12:20 PM         3,634,926 pump_test_features_df.pkl\n",
      "05/31/2018  12:20 PM        14,615,910 pump_train_features_df.pkl\n",
      "05/31/2018  12:20 PM           594,995 pump_train_label_df.pkl\n",
      "06/18/2018  08:19 PM        32,059,977 wp_clean_data.pkl\n",
      "               4 File(s)     50,905,808 bytes\n",
      "               0 Dir(s)  1,927,563,112,448 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir clean-data\\*.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cleaned Water Pump Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive E is DATA\n",
      " Volume Serial Number is 6E98-6AE7\n",
      "\n",
      " Directory of E:\\GitHub\\Udacity\\machine-learning\\projects\\capstone-project\\clean-data\n",
      "\n",
      "06/26/2018  05:53 PM    <DIR>          .\n",
      "06/26/2018  05:53 PM    <DIR>          ..\n",
      "06/13/2018  01:14 PM        25,569,696 clean_pump_train_features_df.pkl\n",
      "06/19/2018  01:26 PM        32,059,792 clean_wp_data_object_18061913.pkl\n",
      "06/16/2018  03:04 PM         5,988,228 clean_wp_test_features_df.pkl\n",
      "06/26/2018  05:53 PM         7,512,042 clean_wp_test_features_df_18062617.pkl\n",
      "06/16/2018  03:04 PM        25,580,661 clean_wp_train_features_df.pkl\n",
      "06/26/2018  05:53 PM        56,455,561 clean_wp_train_features_df_18062617.pkl\n",
      "               6 File(s)    153,165,980 bytes\n",
      "               2 Dir(s)  1,927,563,112,448 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir clean-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Floydhub script: $ floyd run \"Pump it Up - Optimize Model Parameters.ipynb\" --cpu2 --data cmc265/datasets/clean_wp_train_features_df_18062617pkl --mode jupyter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59400, 873)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_file_path = 'clean-data\\\\clean_wp_train_features_df_18062617.pkl'\n",
    "floydhub_file_path = 'cmc265/datasets/clean_wp_train_features_df_18062617pkl'\n",
    "wp_test_file = local_file_path\n",
    "\n",
    "test_df = pd.read_pickle(wp_test_file)\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training and test sets to evaluate models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 41580 samples.\n",
      "Testing set has 17820 samples.\n"
     ]
    }
   ],
   "source": [
    "label_col = 'status_group'\n",
    "target_df = test_df[label_col]\n",
    "feature_df = test_df.drop(label_col, axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, target_df, test_size = 0.3)\n",
    "\n",
    "# Show the results of the split\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune two ensemble models on the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune the Random Forest Model\n",
    "Use a gridsearch with key model hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc= RandomForestClassifier(class_weight = 'balanced', n_jobs=-1)\n",
    "\n",
    "# n_estimators=10, criterion=’gini’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "# max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "# bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)\n",
    "\n",
    "parameters = {'n_estimators': [500, 1000, 2000], 'max_depth': [5, 10, 15,20], \n",
    "              'max_features': ['auto', 'sqrt'], 'min_samples_leaf': [1, 2, 4],\n",
    "              'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "# to better manage imbalance outcome classes \n",
    "sss_cv = StratifiedShuffleSplit(n_splits = 3, test_size=0.3, random_state=0)\n",
    "\n",
    "# Make an f1_score scoring object using make_scorer()\n",
    "scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj =  GridSearchCV(estimator = rfc, param_grid = parameters, scoring = scorer, n_jobs=-1, cv = sss_cv)\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator for model RandomForestClassifier => RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=20, max_features='sqrt',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=1000, n_jobs=-1, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "Best parameters for modelRandomForestClassifier => {'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 1000, 'max_depth': 20, 'min_samples_leaf': 1}\n",
      "Best score RandomForestClassifier => 0.684289729915\n"
     ]
    }
   ],
   "source": [
    "model_name = rfc.__class__.__name__\n",
    "print(\"Best estimator for model {} => {}\".format(model_name, grid_fit.best_estimator_))\n",
    "print(\"Best parameters for model{} => {}\".format(model_name, grid_fit.best_params_))\n",
    "print(\"Best score {} => {}\".format(model_name, grid_fit.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune the Gardient Boosting Model\n",
    "Use a gridsearch with key model hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingClassifier()\n",
    "\n",
    "# loss=’deviance’, learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=’friedman_mse’, \n",
    "# min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, \n",
    "# min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, \n",
    "# warm_start=False, presort=’auto’\n",
    "\n",
    "parameters = {'n_estimators': [100, 500, 1000, 1500], 'learning_rate': [0.01, 0.05, 0.1], \n",
    "              'max_depth': [5, 10, 15] , 'min_samples_leaf': [1, 2, 4],\n",
    "              'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "# to better manage imbalance outcome classes \n",
    "sss_cv = StratifiedShuffleSplit(n_splits = 3, test_size=0.3, random_state=0)\n",
    "\n",
    "# Make an f1_score scoring object using make_scorer()\n",
    "scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj =  GridSearchCV(estimator = gbm, param_grid = parameters, scoring = scorer, n_jobs=-1, cv = sss_cv)\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = gbm.__class__.__name__\n",
    "print(\"Best estimator for model {} => {}\".format(model_name, grid_fit.best_estimator_))\n",
    "print(\"Best parameters for model{} => {}\".format(model_name, grid_fit.best_params_))\n",
    "print(\"Best score {} => {}\".format(model_name, grid_fit.best_score_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
